{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6fa3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "##1. Random Sampling: Each of you will use a random sample of 10K instances drawn from the 61K instances in the secondary mushroom data. Briefly explain the random sampling code cell(s) in your submission notebook. [4 marks]\n",
    "##2. Exploratory Data Analysis (EDA): Create appropriate visualisations to explore your dataset and summarise your findings about the data. Highlight your findings relevant to the model fitting stage (task 3 onwards). [10 marks]\n",
    "##3. Model Shortlisting based on EDA: Based on your findings from the above EDA task, shortlist three classifiers from the classifiers you learnt in the lab classes. Explain your choice of classifiers in terms of your findings from the above EDA task. [5 marks]\n",
    "##4. Model Fitting: Fit the chosen three classifiers to your sample of data, briefly explaining your choices and assumptions. [10 marks]\n",
    "##5. Model Evaluation & model selection: Evaluate your three classifiers from the above task using the cross-validation method and explain the performance of your three classifiers. Explain how you use the cross-validation results to select the ‘winning’ classifier among the three. [15 marks]\n",
    "##6. Final model Selection: In your lab classes, you learnt several classifiers, more than the three you selected in task 3 above. Now fit the remaining classifiers you learnt (excluding the three you already fitted in task 4) and evaluate all the new models fitted in this task using the cross- validation method. Explain how you use the cross-validation results from tasks 5 and 6 to select the final ‘winning’ classifier for your dataset. [15 marks]\n",
    "##7. You have two ‘winning’ classifiers from tasks 5 and 6. Using the evaluation data from tasks 5 and 6 and your findings from the EDA in task 2, explain how helpful your EDA findings have been in improving the efficiency of the model fitting and selection process. [10 marks]\n",
    "##8. Explain the top three lessons/insights you gained from tasks 1 to 6 in building classification models. [6 marks]\n",
    "\n",
    "## all of my explanations will be in one big text file at the end of this notebook, this is to ensure i stayed within the word count and to ensure readability for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74cb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## task 1 \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "primary = pd.read_csv(\"MushroomDataset/primary_data.csv\", sep=\";\")\n",
    "secondary = pd.read_csv(\"MushroomDataset/secondary_data.csv\", sep=\";\")\n",
    "sample.columns = sample.columns.str.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Primary shape:\", primary.shape)\n",
    "print(\"Secondary shape:\", secondary.shape)\n",
    "df = secondary\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "sample_simple = df.sample(n=10000, random_state=42,replace=False )\n",
    "sample_stratified,_= train_test_split(df, train_size=10000, stratify=df['class'], random_state=42)\n",
    "\n",
    "sample = sample_stratified\n",
    "sample.to_csv('mushroom_sample_10k.csv', index=False)\n",
    "print(\"sample shape: \", sample.shape)\n",
    "print(\"sample class counts: \", sample['class'].value_counts(normalize=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## class distribution\n",
    "plt.figure(figsize=(6,4))\n",
    "ax = sns.countplot(x='class', data=sample)\n",
    "\n",
    "total = len(sample)\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    percentage = height / total * 100\n",
    "    ax.annotate(f'{percentage:.1f}%', (p.get_x() + p.get_width() / 2., height),\n",
    "                ha='center', va='bottom')\n",
    "plt.show()\n",
    "\n",
    "## categorical feature distributions\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    ax = sns.countplot(y=col, data=sample, order=sample[col].value_counts().index)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    \n",
    "    # Add percentages on top of bars for comparisons\n",
    "    total = len(sample)\n",
    "    for p in ax.patches:\n",
    "        width = p.get_width()\n",
    "        percentage = width / total * 100\n",
    "        ax.annotate(f'{percentage:.1f}%', (width, p.get_y() + p.get_height() / 2.),\n",
    "                    ha='left', va='center')\n",
    "    plt.show()\n",
    "\n",
    "## numeric feature distributions\n",
    "sample[numeric_cols].hist(bins=20, figsize=(10,4))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "##task 3\n",
    "\n",
    "#Based on EDA select 3 classifiers, randomFores, gradientBoosting and logicRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "shortlisted_models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "print(\"Task 3: Shortlisted models:\", list(shortlisted_models.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "##task 4 \n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Define categorical and numeric columns\n",
    "numeric_cols = ['cap-diameter', 'stem-height', 'stem-width']\n",
    "categorical_cols = ['cap-shape', 'cap-surface', 'cap-color', 'gill-color', 'stem-color', 'stem-surface','veil-color', 'ring-type', 'spore-print-color','habitat', 'season', 'does-bruise-or-bleed', 'gill-attachment', 'gill-spacing', 'stem-root', 'veil-type', 'has-ring']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "model_pipelines= {}\n",
    "for name, clf in shortlisted_models.items():\n",
    "    pipe = Pipeline([('preprocess', preprocessor), ('classifier', clf)])\n",
    "    pipe.fit(sample.drop('class', axis=1), sample['class'])\n",
    "    model_pipelines[name] = pipe\n",
    "\n",
    "print(\"Task 4: Models fitted successfully.\")\n",
    "print(model_pipelines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c003cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## task 5\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_results = {}\n",
    "for name, model in model_pipelines.items():\n",
    "    scores = cross_val_score(model, sample.drop('class', axis=1), sample['class'], \n",
    "                             cv=5, scoring='accuracy')\n",
    "    cv_results[name] = scores\n",
    "    print(f\"{name}: Mean CV Accuracy = {scores.mean()*100:.2f}%, Std = {scores.std()*100:.2f}\")\n",
    "    \n",
    "# Select the winning model\n",
    "winning_model_name = max(cv_results, key=lambda k: cv_results[k].mean())\n",
    "print(f\"\\nTask 5: Winning model based on CV = {winning_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef410df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: Mean CV Accuracy = 99.98%, Std = 0.02%\n",
      "NaiveBayes: Mean CV Accuracy = 59.78%, Std = 1.10%\n"
     ]
    }
   ],
   "source": [
    "## Task 6\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "dense_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "additional_models = {\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'NaiveBayes': GaussianNB(),\n",
    "    'SVM': SVC(kernel='rbf', random_state=42),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RidgeClassifier': RidgeClassifier(),\n",
    "    'SGDClassifier': SGDClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "additional_pipelines = {}\n",
    "additional_cv_results = {}\n",
    "\n",
    "for name, clf in additional_models.items():\n",
    "    pipe = Pipeline([('preprocess', dense_preprocessor), ('classifier', clf)])\n",
    "    additional_pipelines[name] = pipe\n",
    "    \n",
    "    scores = cross_val_score(\n",
    "        pipe, sample.drop('class', axis=1), sample['class'],\n",
    "        cv=5, scoring='accuracy'\n",
    "    )\n",
    "    additional_cv_results[name] = scores\n",
    "    print(f\"{name}: Mean CV Accuracy = {scores.mean() * 100:.2f}%, Std = {scores.std() * 100:.2f}%\")\n",
    "\n",
    "all_results = {**cv_results, **additional_cv_results}\n",
    "final_winning_model_name = max(all_results, key=lambda k: all_results[k].mean())\n",
    "\n",
    "print(f\"\\nTask 6: Final winning model across all classifiers = {final_winning_model_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
